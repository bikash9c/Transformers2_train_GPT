---
title: GPT-124M Demo
emoji: ðŸ¤–
colorFrom: blue
colorTo: purple
sdk: gradio
sdk_version: 4.44.0
app_file: app.py
pinned: false
license: mit
---

# GPT-124M Language Model Demo

A decoder-only transformer (GPT-2 architecture) trained from scratch with **124 million parameters**.

## ðŸŽ¯ Achievement

### Training Metrics
- âœ… **Final Training Loss:** 0.0979 (Target: < 0.1) 
- âœ… **Perplexity:** 1.16 (extremely low due to small dataset)
- âœ… **Training Steps:** 521 out of 2000 max (early stopping triggered)
- âœ… **Training Duration:** ~1.8 hours on T4 GPU
- âœ… **Loss Reduction:** 99.1% (from 10.899 â†’ 0.098)

### Training Progress
- **Initial Loss (Step 0):** 10.899
- **Mid Training (Step 250):** ~0.5
- **Final Loss (Step 521):** 0.0979
- **Gradient Norm:** Started at 28.75, stabilized to ~0.7-1.0

## ðŸ—ï¸ Architecture

### Model Specifications
- **Type:** Decoder-only Transformer (GPT-2 style)
- **Total Parameters:** 124,439,808 (~124M)
- **Model Size:** 
  - Float32: 474 MB
  - Float16/BFloat16: 237 MB
  - With Optimizer State: 1.47 GB

### Architecture Details
- **Layers (n_layer):** 12 transformer blocks
- **Attention Heads (n_head):** 12 heads per layer
- **Embedding Dimension (n_embd):** 768
- **Head Dimension:** 64 (768 / 12)
- **MLP Hidden Dimension:** 3,072 (4 Ã— 768)
- **Vocabulary Size:** 50,304 tokens (padded for GPU efficiency)
- **Context Length (block_size):** 1024 tokens
- **Position Embeddings:** Learned (1024 positions)

### Key Components
- **Attention:** Causal self-attention with flash attention optimization
- **Activation:** GELU (tanh approximation)
- **Normalization:** LayerNorm (pre-norm architecture)
- **Residual Scaling:** GPT-2 style residual initialization
- **Weight Tying:** Token embeddings shared with output projection

## ðŸ“Š Training Details

### Dataset
- **Total Tokens:** 338,025 tokens
- **Training Data:** Custom text corpus
- **Epochs Completed:** ~194 epochs (521 steps Ã— 10 steps/epoch)

### Optimization
- **Optimizer:** AdamW (fused version for GPU efficiency)
  - Weight decay: 0.1
  - Betas: (0.9, 0.95)
  - Epsilon: 1e-8
- **Learning Rate Schedule:**
  - Max LR: 6e-4
  - Min LR: 6e-5 (10% of max)
  - Warmup: 200 steps (linear)
  - Decay: Cosine decay after warmup
- **Gradient Clipping:** 1.0 (prevents exploding gradients)

### Batch Configuration
- **Micro Batch Size (B):** 2 sequences
- **Sequence Length (T):** 1024 tokens
- **Tokens per Micro Batch:** 2,048 tokens
- **Gradient Accumulation Steps:** 16
- **Effective Batch Size:** 32,768 tokens (16 Ã— 2,048)
- **Why Gradient Accumulation?** Simulates large batch training while fitting in 15GB GPU memory

### Training Infrastructure
- **Device:** NVIDIA T4 GPU (15GB VRAM)
- **Precision:** Mixed precision (bfloat16) for faster training
- **Memory Usage:** ~4.5 GB (model + optimizer + activations)
- **Throughput:** ~2,660 tokens/second
- **Time per Step:** ~12 seconds
- **Compilation:** torch.compile enabled (2x speedup)

## ðŸŽ® Features

### 1. Text Generation
- Interactive text continuation from custom prompts
- **Adjustable Parameters:**
  - Max Tokens: 10-200 (default: 90)
  - Temperature: 0.1-2.0 (default: 0.5)
  - Top-K Sampling: 0-100 (default: 10)
- Real-time generation with progress tracking

### 2. Next Token Prediction
- See model's top predictions with confidence scores
- Displays probability distribution for next token
- Helpful for understanding model behavior

### 3. Model Information
- Complete architecture specifications
- Training metrics and hyperparameters
- Performance statistics

## ðŸ“ Important Notes

### Model Behavior
This model was trained on a **very small dataset (338K tokens)** - approximately 160 pages of text. As a result:

- âœ… **Successfully achieved the training objective** (loss < 0.1)
- âœ… **Training pipeline works correctly** (optimized for 124M parameters)
- âš ï¸ **Generation quality is limited** due to dataset size
- âš ï¸ **Model has memorized training patterns** rather than learning general language

### Expected Performance
- **Perplexity of 1.16** indicates near-perfect memorization of training data
- Generated text may contain fragments from training set
- Output may be incoherent or nonsensical on novel prompts
- This is **expected behavior** for such a small training corpus

### For Production Use
To achieve coherent, general-purpose text generation:
- **Recommended dataset size:** 100M - 10B tokens
- **Expected training time:** Days to weeks
- **Hardware requirements:** Multiple GPUs
- **Typical perplexity:** 15-30 for good models

### What This Demo Proves
1. âœ… Complete training pipeline from scratch
2. âœ… Proper gradient descent and optimization
3. âœ… Model architecture implementation
4. âœ… GPU acceleration and mixed precision training
5. âœ… Checkpoint saving and model deployment

## ðŸ› ï¸ Technical Stack

### Core Dependencies
- **PyTorch 2.1.0** - Deep learning framework
- **Gradio 4.44.0** - Web UI framework
- **Tiktoken 0.5.1** - GPT-2 tokenizer

### Training Optimizations
- **Flash Attention** - Optimized attention computation
- **Torch Compile** - JIT compilation for 2x speedup
- **Fused AdamW** - Optimized optimizer kernel
- **Gradient Accumulation** - Large batch simulation
- **Mixed Precision (BFloat16)** - Faster training, reduced memory
- **Gradient Clipping** - Training stability

### Model File
- **Checkpoint Size:** 1.47 GB (includes optimizer state)
- **Model Only:** 474 MB (float32) or 237 MB (float16)
- **Format:** PyTorch state_dict

## ðŸ“„ License
MIT

---

## ðŸ“Š Training Logs

<details>
<summary><b>Click to expand full training logs (521 steps)</b></summary>

```
using fused AdamW: True
step    0 | loss: 10.899050 | lr: 3.0000e-06 | norm: 28.7510 | dt: 11911.88ms | tok/sec: 2750.87
step    1 | loss: 10.607647 | lr: 6.0000e-06 | norm: 24.5606 | dt: 11808.95ms | tok/sec: 2774.84
step    2 | loss: 10.145452 | lr: 9.0000e-06 | norm: 17.6338 | dt: 12011.94ms | tok/sec: 2727.95
...
step   50 | loss: 6.183827 | lr: 1.5300e-04 | norm: 1.6772 | dt: 12273.63ms | tok/sec: 2669.79
...
step  100 | loss: 4.891646 | lr: 3.0300e-04 | norm: 1.5395 | dt: 12297.58ms | tok/sec: 2664.59
...
step  150 | loss: 4.196229 | lr: 4.5300e-04 | norm: 1.4892 | dt: 12290.60ms | tok/sec: 2666.10
...
step  200 | loss: 3.719590 | lr: 6.0000e-04 | norm: 2.4772 | dt: 12312.17ms | tok/sec: 2661.43
...
step  250 | loss: 3.069603 | lr: 5.9897e-04 | norm: 2.1632 | dt: 12312.91ms | tok/sec: 2661.27
...
step  300 | loss: 2.445577 | lr: 5.9590e-04 | norm: 3.1147 | dt: 12317.30ms | tok/sec: 2660.32
...
step  350 | loss: 1.752725 | lr: 5.9080e-04 | norm: 2.7623 | dt: 12341.65ms | tok/sec: 2655.08
...
step  400 | loss: 0.960184 | lr: 5.8372e-04 | norm: 1.8737 | dt: 12313.77ms | tok/sec: 2661.09
...
step  450 | loss: 0.516292 | lr: 5.7470e-04 | norm: 1.4155 | dt: 12327.27ms | tok/sec: 2658.17
...
step  500 | loss: 0.187996 | lr: 5.6383e-04 | norm: 0.9739 | dt: 12344.16ms | tok/sec: 2654.54
step  501 | loss: 0.184034 | lr: 5.6359e-04 | norm: 1.0193 | dt: 12344.86ms | tok/sec: 2654.38
step  502 | loss: 0.172398 | lr: 5.6335e-04 | norm: 0.8906 | dt: 12328.70ms | tok/sec: 2657.86
step  503 | loss: 0.165317 | lr: 5.6312e-04 | norm: 0.9336 | dt: 12315.54ms | tok/sec: 2660.70
step  504 | loss: 0.155701 | lr: 5.6288e-04 | norm: 0.9689 | dt: 12300.39ms | tok/sec: 2663.98
step  505 | loss: 0.227812 | lr: 5.6264e-04 | norm: 1.2178 | dt: 12315.22ms | tok/sec: 2660.77
step  506 | loss: 0.214637 | lr: 5.6240e-04 | norm: 1.0463 | dt: 12323.61ms | tok/sec: 2658.96
step  507 | loss: 0.154780 | lr: 5.6216e-04 | norm: 0.8291 | dt: 12325.33ms | tok/sec: 2658.59
step  508 | loss: 0.136490 | lr: 5.6192e-04 | norm: 0.7597 | dt: 12351.53ms | tok/sec: 2652.95
step  509 | loss: 0.148413 | lr: 5.6168e-04 | norm: 0.9156 | dt: 12338.93ms | tok/sec: 2655.66
step  510 | loss: 0.134263 | lr: 5.6144e-04 | norm: 0.8513 | dt: 12357.40ms | tok/sec: 2651.69
step  511 | loss: 0.141122 | lr: 5.6119e-04 | norm: 0.8578 | dt: 12358.42ms | tok/sec: 2651.47
step  512 | loss: 0.150347 | lr: 5.6095e-04 | norm: 0.8816 | dt: 12321.05ms | tok/sec: 2659.51
step  513 | loss: 0.146648 | lr: 5.6070e-04 | norm: 0.8443 | dt: 12294.91ms | tok/sec: 2665.17
step  514 | loss: 0.122815 | lr: 5.6046e-04 | norm: 0.7679 | dt: 12310.32ms | tok/sec: 2661.83
step  515 | loss: 0.139969 | lr: 5.6021e-04 | norm: 0.8348 | dt: 12345.18ms | tok/sec: 2654.31
step  516 | loss: 0.168378 | lr: 5.5997e-04 | norm: 1.0398 | dt: 12348.56ms | tok/sec: 2653.59
step  517 | loss: 0.118655 | lr: 5.5972e-04 | norm: 0.7890 | dt: 12342.00ms | tok/sec: 2655.00
step  518 | loss: 0.109324 | lr: 5.5947e-04 | norm: 0.7787 | dt: 12328.69ms | tok/sec: 2657.87
step  519 | loss: 0.130563 | lr: 5.5922e-04 | norm: 0.8226 | dt: 12314.30ms | tok/sec: 2660.97
step  520 | loss: 0.115266 | lr: 5.5897e-04 | norm: 0.7358 | dt: 12312.35ms | tok/sec: 2661.39
step  521 | loss: 0.097898 | lr: 5.5872e-04 | norm: 0.7436 | dt: 12313.36ms | tok/sec: 2661.17

ðŸŽ‰ Achieved loss < 0.1 at step 521!

Final loss: 0.097898
```

### Key Milestones:
- **Step 0:** Loss 10.899 (initial random state)
- **Step 50:** Loss 6.184 (43% reduction)
- **Step 100:** Loss 4.892 (55% reduction)
- **Step 200:** Loss 3.720 (66% reduction)
- **Step 300:** Loss 2.446 (78% reduction)
- **Step 400:** Loss 0.960 (91% reduction)
- **Step 500:** Loss 0.188 (98% reduction)
- **Step 521:** Loss 0.098 âœ… **TARGET ACHIEVED!**

### Training Statistics:
- **Total training time:** ~1.8 hours (6,427 seconds)
- **Average time per step:** 12.3 seconds
- **Average throughput:** 2,660 tokens/second
- **GPU memory utilization:** ~4.5 GB / 15 GB
- **Gradient norm progression:** 28.75 â†’ 0.74 (stable training)

</details>
